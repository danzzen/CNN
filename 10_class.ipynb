{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = unpickle('cifar-10-batches-py/test_batch')\n",
    "# print(tt[b'data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import all modules\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "batch_size = 32 \n",
    "# 32 examples in a mini-batch, smaller batch size means more updates in one epoch\n",
    " \n",
    "num_classes = 10 #\n",
    "epochs =10 # repeat 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "(32, 32, 3)\n",
      "(50000,)\n",
      "(10000, 32, 32, 3)\n",
      "(10000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKRK/EJki8QuRKRK/EJki8QuRKf8PDmWr6qljPucAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21f3d0e5198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_cifar10_data(data_dir):\n",
    " '''Return train_data, train_labels, test_data, test_labels\n",
    " The shape of data is 32 x 32 x3'''\n",
    " train_data = None\n",
    " train_labels = []\n",
    "\n",
    " for i in range(1, 6):\n",
    "  data_dic = unpickle(data_dir + \"/data_batch_{}\".format(i))\n",
    "  if i == 1:\n",
    "   train_data = data_dic[b'data']\n",
    "  else:\n",
    "   train_data = np.vstack((train_data, data_dic[b'data']))\n",
    "  train_labels += data_dic[b'labels']\n",
    "\n",
    " test_data_dic = unpickle(data_dir + \"/test_batch\")\n",
    " test_data = test_data_dic[b'data']\n",
    " test_labels = test_data_dic[b'labels']\n",
    "\n",
    " train_data = train_data.reshape((len(train_data), 3, 32,32))\n",
    "#  print(train_data)\n",
    " train_data = np.rollaxis(train_data, 1, 4)\n",
    "#  print(train_data)\n",
    " train_labels = np.array(train_labels)\n",
    "\n",
    " test_data = test_data.reshape((len(test_data),3, 32,32))\n",
    " print(len(test_data))\n",
    " test_data = np.rollaxis(test_data, 1, 4)\n",
    " test_labels = np.array(test_labels)\n",
    "\n",
    " return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "data_dir = 'cifar-10-batches-py'\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(data_dir)\n",
    "\n",
    "print(train_data.shape[1:])\n",
    "print(train_labels.shape)\n",
    "\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "# In order to check where the data shows an image correctly\n",
    "plt.imshow(train_data[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n"
     ]
    }
   ],
   "source": [
    "# print(test_labels.tolist())\n",
    "print(train_data.dtype)\n",
    "\n",
    " \n",
    "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = np_utils.to_categorical(test_labels, num_classes)\n",
    "train_data = train_data.astype('float32')\n",
    "test_data = test_data.astype('float32')\n",
    "train_data=train_data/255\n",
    "test_data=test_data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_107 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_108 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 4,210,090\n",
      "Trainable params: 4,210,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 461s 9ms/step - loss: 1.7195 - acc: 0.3784 - val_loss: 1.4395 - val_acc: 0.4826\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 459s 9ms/step - loss: 1.3643 - acc: 0.5103 - val_loss: 1.2163 - val_acc: 0.5669\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 467s 9ms/step - loss: 1.2230 - acc: 0.5622 - val_loss: 1.1389 - val_acc: 0.5930\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 459s 9ms/step - loss: 1.1383 - acc: 0.5973 - val_loss: 1.1073 - val_acc: 0.6074\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 459s 9ms/step - loss: 1.0680 - acc: 0.6200 - val_loss: 1.0638 - val_acc: 0.6266\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 460s 9ms/step - loss: 1.0154 - acc: 0.6408 - val_loss: 1.0377 - val_acc: 0.6335\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 951s 19ms/step - loss: 0.9688 - acc: 0.6591 - val_loss: 1.0181 - val_acc: 0.6424\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 553s 11ms/step - loss: 0.9311 - acc: 0.6715 - val_loss: 1.0070 - val_acc: 0.6432\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 564s 11ms/step - loss: 0.8936 - acc: 0.6867 - val_loss: 0.9910 - val_acc: 0.6529\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 524s 10ms/step - loss: 0.8601 - acc: 0.6972 - val_loss: 0.9826 - val_acc: 0.6573\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(32, 32,3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "epochs = 10\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# Fit model\n",
    "\n",
    "cnn = model.fit(train_data, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(test_data,test_labels),shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 59,  62,  63],\n",
       "         [ 43,  46,  45],\n",
       "         [ 50,  48,  43],\n",
       "         ..., \n",
       "         [158, 132, 108],\n",
       "         [152, 125, 102],\n",
       "         [148, 124, 103]],\n",
       "\n",
       "        [[ 16,  20,  20],\n",
       "         [  0,   0,   0],\n",
       "         [ 18,   8,   0],\n",
       "         ..., \n",
       "         [123,  88,  55],\n",
       "         [119,  83,  50],\n",
       "         [122,  87,  57]],\n",
       "\n",
       "        [[ 25,  24,  21],\n",
       "         [ 16,   7,   0],\n",
       "         [ 49,  27,   8],\n",
       "         ..., \n",
       "         [118,  84,  50],\n",
       "         [120,  84,  50],\n",
       "         [109,  73,  42]],\n",
       "\n",
       "        ..., \n",
       "        [[208, 170,  96],\n",
       "         [201, 153,  34],\n",
       "         [198, 161,  26],\n",
       "         ..., \n",
       "         [160, 133,  70],\n",
       "         [ 56,  31,   7],\n",
       "         [ 53,  34,  20]],\n",
       "\n",
       "        [[180, 139,  96],\n",
       "         [173, 123,  42],\n",
       "         [186, 144,  30],\n",
       "         ..., \n",
       "         [184, 148,  94],\n",
       "         [ 97,  62,  34],\n",
       "         [ 83,  53,  34]],\n",
       "\n",
       "        [[177, 144, 116],\n",
       "         [168, 129,  94],\n",
       "         [179, 142,  87],\n",
       "         ..., \n",
       "         [216, 184, 140],\n",
       "         [151, 118,  84],\n",
       "         [123,  92,  72]]],\n",
       "\n",
       "\n",
       "       [[[154, 177, 187],\n",
       "         [126, 137, 136],\n",
       "         [105, 104,  95],\n",
       "         ..., \n",
       "         [ 91,  95,  71],\n",
       "         [ 87,  90,  71],\n",
       "         [ 79,  81,  70]],\n",
       "\n",
       "        [[140, 160, 169],\n",
       "         [145, 153, 154],\n",
       "         [125, 125, 118],\n",
       "         ..., \n",
       "         [ 96,  99,  78],\n",
       "         [ 77,  80,  62],\n",
       "         [ 71,  73,  61]],\n",
       "\n",
       "        [[140, 155, 164],\n",
       "         [139, 146, 149],\n",
       "         [115, 115, 112],\n",
       "         ..., \n",
       "         [ 79,  82,  64],\n",
       "         [ 68,  70,  55],\n",
       "         [ 67,  69,  55]],\n",
       "\n",
       "        ..., \n",
       "        [[175, 167, 166],\n",
       "         [156, 154, 160],\n",
       "         [154, 160, 170],\n",
       "         ..., \n",
       "         [ 42,  34,  36],\n",
       "         [ 61,  53,  57],\n",
       "         [ 93,  83,  91]],\n",
       "\n",
       "        [[165, 154, 128],\n",
       "         [156, 152, 130],\n",
       "         [159, 161, 142],\n",
       "         ..., \n",
       "         [103,  93,  96],\n",
       "         [123, 114, 120],\n",
       "         [131, 121, 131]],\n",
       "\n",
       "        [[163, 148, 120],\n",
       "         [158, 148, 122],\n",
       "         [163, 156, 133],\n",
       "         ..., \n",
       "         [143, 133, 139],\n",
       "         [143, 134, 142],\n",
       "         [143, 133, 144]]],\n",
       "\n",
       "\n",
       "       [[[255, 255, 255],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         ..., \n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ..., \n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ..., \n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254]],\n",
       "\n",
       "        ..., \n",
       "        [[113, 120, 112],\n",
       "         [111, 118, 111],\n",
       "         [105, 112, 106],\n",
       "         ..., \n",
       "         [ 72,  81,  80],\n",
       "         [ 72,  80,  79],\n",
       "         [ 72,  80,  79]],\n",
       "\n",
       "        [[111, 118, 110],\n",
       "         [104, 111, 104],\n",
       "         [ 99, 106,  98],\n",
       "         ..., \n",
       "         [ 68,  75,  73],\n",
       "         [ 70,  76,  75],\n",
       "         [ 78,  84,  82]],\n",
       "\n",
       "        [[106, 113, 105],\n",
       "         [ 99, 106,  98],\n",
       "         [ 95, 102,  94],\n",
       "         ..., \n",
       "         [ 78,  85,  83],\n",
       "         [ 79,  85,  83],\n",
       "         [ 80,  86,  84]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 35, 178, 235],\n",
       "         [ 40, 176, 239],\n",
       "         [ 42, 176, 241],\n",
       "         ..., \n",
       "         [ 99, 177, 219],\n",
       "         [ 79, 147, 197],\n",
       "         [ 89, 148, 189]],\n",
       "\n",
       "        [[ 57, 182, 234],\n",
       "         [ 44, 184, 250],\n",
       "         [ 50, 183, 240],\n",
       "         ..., \n",
       "         [156, 182, 200],\n",
       "         [141, 177, 206],\n",
       "         [116, 149, 175]],\n",
       "\n",
       "        [[ 98, 197, 237],\n",
       "         [ 64, 189, 252],\n",
       "         [ 69, 192, 245],\n",
       "         ..., \n",
       "         [188, 195, 206],\n",
       "         [119, 135, 147],\n",
       "         [ 61,  79,  90]],\n",
       "\n",
       "        ..., \n",
       "        [[ 73,  79,  77],\n",
       "         [ 53,  63,  68],\n",
       "         [ 54,  68,  80],\n",
       "         ..., \n",
       "         [ 17,  40,  64],\n",
       "         [ 21,  36,  51],\n",
       "         [ 33,  48,  49]],\n",
       "\n",
       "        [[ 61,  68,  75],\n",
       "         [ 55,  70,  86],\n",
       "         [ 57,  79, 103],\n",
       "         ..., \n",
       "         [ 24,  48,  72],\n",
       "         [ 17,  35,  53],\n",
       "         [  7,  23,  32]],\n",
       "\n",
       "        [[ 44,  56,  73],\n",
       "         [ 46,  66,  88],\n",
       "         [ 49,  77, 105],\n",
       "         ..., \n",
       "         [ 27,  52,  77],\n",
       "         [ 21,  43,  66],\n",
       "         [ 12,  31,  50]]],\n",
       "\n",
       "\n",
       "       [[[189, 211, 240],\n",
       "         [186, 208, 236],\n",
       "         [185, 207, 235],\n",
       "         ..., \n",
       "         [175, 195, 224],\n",
       "         [172, 194, 222],\n",
       "         [169, 194, 220]],\n",
       "\n",
       "        [[194, 210, 239],\n",
       "         [191, 207, 236],\n",
       "         [190, 206, 235],\n",
       "         ..., \n",
       "         [173, 192, 220],\n",
       "         [171, 191, 218],\n",
       "         [167, 190, 216]],\n",
       "\n",
       "        [[208, 219, 244],\n",
       "         [205, 216, 240],\n",
       "         [204, 215, 239],\n",
       "         ..., \n",
       "         [175, 191, 217],\n",
       "         [172, 190, 216],\n",
       "         [169, 191, 215]],\n",
       "\n",
       "        ..., \n",
       "        [[207, 199, 181],\n",
       "         [203, 195, 175],\n",
       "         [203, 196, 173],\n",
       "         ..., \n",
       "         [135, 132, 127],\n",
       "         [162, 158, 150],\n",
       "         [168, 163, 151]],\n",
       "\n",
       "        [[198, 190, 170],\n",
       "         [189, 181, 159],\n",
       "         [180, 172, 147],\n",
       "         ..., \n",
       "         [178, 171, 160],\n",
       "         [175, 169, 156],\n",
       "         [175, 169, 154]],\n",
       "\n",
       "        [[198, 189, 173],\n",
       "         [189, 181, 162],\n",
       "         [178, 170, 149],\n",
       "         ..., \n",
       "         [195, 184, 169],\n",
       "         [196, 189, 171],\n",
       "         [195, 190, 171]]],\n",
       "\n",
       "\n",
       "       [[[229, 229, 239],\n",
       "         [236, 237, 247],\n",
       "         [234, 236, 247],\n",
       "         ..., \n",
       "         [217, 219, 233],\n",
       "         [221, 223, 234],\n",
       "         [222, 223, 233]],\n",
       "\n",
       "        [[222, 221, 229],\n",
       "         [239, 239, 249],\n",
       "         [233, 234, 246],\n",
       "         ..., \n",
       "         [223, 223, 236],\n",
       "         [227, 228, 238],\n",
       "         [210, 211, 220]],\n",
       "\n",
       "        [[213, 206, 211],\n",
       "         [234, 232, 239],\n",
       "         [231, 233, 244],\n",
       "         ..., \n",
       "         [220, 220, 232],\n",
       "         [220, 219, 232],\n",
       "         [202, 203, 215]],\n",
       "\n",
       "        ..., \n",
       "        [[150, 143, 135],\n",
       "         [140, 135, 127],\n",
       "         [132, 127, 120],\n",
       "         ..., \n",
       "         [224, 222, 218],\n",
       "         [230, 228, 225],\n",
       "         [241, 241, 238]],\n",
       "\n",
       "        [[137, 132, 126],\n",
       "         [130, 127, 120],\n",
       "         [125, 121, 115],\n",
       "         ..., \n",
       "         [181, 180, 178],\n",
       "         [202, 201, 198],\n",
       "         [212, 211, 207]],\n",
       "\n",
       "        [[122, 119, 114],\n",
       "         [118, 116, 110],\n",
       "         [120, 116, 111],\n",
       "         ..., \n",
       "         [179, 177, 173],\n",
       "         [164, 164, 162],\n",
       "         [163, 163, 161]]]], dtype=uint8)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.73%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_data, test_labels, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now using whitening \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
